---
title: "My title"
subtitle: "My subtitle if needed"
author: 
  - Yiyi Yao
thanks: "Code and data are available at: [https://github.com/Yaoee111/condo_evaluation]."
date: 03 December, 2024
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(knitr)
library(kableExtra)
library(rstanarm)
library(modelsummary)
library(tidybayes)
```

```{r}
#| include: false
#| warning: false
#| message: false

data <- read.csv("/cloud/project/data/02-analysis_data/toronto_apartment_evaluation.csv")
```


# Introduction

Overview paragraph

Estimand paragraph

Results paragraph

Why it matters paragraph

Telegraphing paragraph: The remainder of this paper is structured as follows. @sec-data....

# Data {#sec-data}

We use the statistical programming language R [@citeR] with packages `tidyverse` \[\@\], `ggplot2` \[\@\], Follow @tellingstories.

## Data source

Data for this research comes from RentSafeTO program provided by Open Data Toronto (@opendata). This dataset contains building evaluation scores for apartment buildings registered with the RentSafeTO program in Toronto until 2023. RentSafeTO (@rentsafeto_tool), established in 2017, is a bylaw enforcement initiative designed to ensure that apartment buildings with three or more storeys or at least 10 units meet required maintenance standards. Alongside evaluation details, the dataset provides information about the buildings, including their address, year built, and geographical coordinates. The RentSafeTO dataset was selected because it provides a comprehensive overview of apartment building maintenance standards in Toronto.

## Data Features

The original RentSafeTo dataset, which shows in \@ in Appendix @sec-data-details, contains 3573 data entries and many variables. Since we only interested in data in downtown Toronto, this report will only explore and analyze through several data features. We chose these 10 variables: RSN, WARD, WARDNAME, YEAR.BUILT, PROPERTY.TYPE, CONFIRMED.STOREYS, CONFIRMED.UNITS, CURRENT.BUILDING.EVAL.SCORE, PROACTIVE.BUILDING.SCORE, CURRENT.REACTIVE.SCORE. Details of these selected variables are in @tbl-feature. We could use such features to predict building condition in downtown Toronto.

## Data Measurement

The data in this dataset was collected by bylaw enforcement officers as part of the RentSafeTO program. These inspections evaluate building maintenance standards across various categories, such as exterior grounds, mechanical systems, and common areas. Scores are assigned on a scale of 1 to 3, with 1 being the lowest and 3 being the highest. A score of 0 is given when an item cannot be evaluated due to obstruction or refusal, and blanks are recorded when an item is not applicable.

The unit of measurement for most numerical data is an ordinal score (1 to 3). Other features, such as building types or ward names, are categorical with no specific units.

## Data limitation

The data represents evaluations of buildings registered under RentSafeTO, so it does not cover all apartment buildings in Toronto. This limits its ability to represent the full state of rental housing in the city. Additionally, the evaluation process can be affected by variability in inspector judgments, leading to potential inconsistencies in scores. Gaps in the data, such as 0 scores or blanks, further complicate analysis and reduce the dataset's completeness. Moreover, the dataset primarily focuses on physical and structural conditions, neglecting tenant perspectives or broader social factors that may influence overall building quality. These limitations highlight the need to interpret the data with caution when making broader inferences.

## Data Methodology

The original dataset contains information on all registered apartment buildings in the Toronto area, along with the data needed for their evaluation and scoring. This comprehensive data includes various attributes related to building conditions and maintenance. However, since our primary research objective is to predict the conditions of apartment buildings specifically in the downtown area, we narrowed our focus to the variables most relevant to this goal. To achieve this, we carefully selected key features, like YEAR.BUILT, PROPERTY.TYPE, CONFIRMED.UNITS, CURRENT.BUILDING.EVAL.SCORE, and clean the original dataset, ensuring it aligns with the scope and requirements of our analysis.

## Data Visualization

@fig-data1 illustrates the relationship between CURRENT.BUILDING.EVAL.SCORE and YEAR.BUILT. Each point represents a building, with its year of construction on the x-axis and its evaluation score on the y-axis. The plot includes a trend line generated through linear regression to highlight any general trends. The positive trend line suggests that buildings constructed more recently tend to have slightly higher evaluation scores, although the relationship is not very strong. This could indicate that newer buildings are generally better maintained or constructed to meet higher standards. However, there is considerable variability in the scores for buildings of all ages, as shown by the spread of points around the trend line.

Outliers in the older buildings with high scores could reflect cases where older properties have been well-maintained or renovated. Conversely, some newer buildings with lower scores might indicate specific issues despite their recent construction. This graph highlights the importance of maintenance and management practices in addition to construction year.

```{r}
#| label: fig-data1
#| tbl-cap: "Relationship between Evaluation Score and Year Built"
#| message: false
#| echo: false
#| warning: false

ggplot(data, aes(x = YEAR.BUILT, y = CURRENT.BUILDING.EVAL.SCORE)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, col = "blue") +
  labs(
    x = "Year Built",
    y = "Current Building Evaluation Score"
  )
```

@fig-data2 compares CURRENT.BUILDING.EVAL.SCORE across different PROPERTY.TYPE categories. Each box represents the distribution of scores for a specific property type (e.g., Private, Social Housing, TCHC). The graph shows the median score as well as the range of scores within each category. All property types have relatively high median scores, but slight variations can be observed. TCHC and Social Housing buildings have marginally higher medians than Private properties, suggesting potentially better maintenance or stricter compliance standards. Private properties exhibit the largest range of scores, with several outliers at the lower end. This indicates that some private buildings may significantly underperform in terms of maintenance and compliance. Social Housing and TCHC properties have narrower score distributions, implying more consistent maintenance or management practices. The lower outliers in Private properties highlight a subset of buildings that fall well below the typical evaluation scores, potentially requiring closer attention or intervention.

The graph suggests that Social Housing and TCHC properties are generally more consistent in maintaining standards, while Private properties show greater variability and occasional poor performance.

```{r}
#| label: fig-data2
#| tbl-cap: "Evaluation Score by Property Type"
#| message: false
#| echo: false
#| warning: false

ggplot(data, aes(x = PROPERTY.TYPE, y = CURRENT.BUILDING.EVAL.SCORE)) +
  geom_boxplot() +
  labs(
    x = "Property Type",
    y = "Current Building Evaluation Score"
  )
```

@fig-data3 shows the relationship between CURRENT.BUILDING.EVAL.SCORE and CONFIRMED.UNITS. The x-axis represents the number of units in a building, while the y-axis shows the evaluation score. A regression trend line is included to illustrate any patterns in the data. The line shows a slight upward trend, suggesting that buildings with more units tend to have marginally higher evaluation scores. This could indicate that larger buildings might have more structured management systems, leading to better maintenance and compliance. There is a significant spread of scores across all building sizes, with no strong clustering or consistency. This suggests that the number of units alone does not strongly predict evaluation scores, and other factors, such as management quality or age of the building, might play a more prominent role. A few buildings with lower scores, regardless of their number of units, could highlight individual cases where maintenance or compliance was poor, despite size advantages.

While larger buildings may have slightly better evaluation scores on average, the overall variability indicates that the size of a building is not a decisive factor in determining its condition or compliance. This suggests the need for a more nuanced analysis considering additional variables.

```{r}
#| label: fig-data3
#| tbl-cap: "Relationship between Evaluation Score and Number of Units"
#| message: false
#| echo: false
#| warning: false

ggplot(data, aes(x = CONFIRMED.UNITS, y = CURRENT.BUILDING.EVAL.SCORE)) +
  geom_point() +
  geom_smooth(method = "lm", col = "red") +
  labs(
    x = "Number of Units",
    y = "Current Building Evaluation Score"
  )
```



# Model {#sec-model}

In our analysis, we utilized a Bayesian logistic regression model to examine the relationship between apartment evaluation and other building factors ---- built year, number of units, and location of building (ward). Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

The model is formulated as follows:

```{=tex}
\begin{align} 
\text{SCORE}_i &\sim \mbox{Normal}(\mu_i, \sigma^2) \\
\mu_i &= \beta_0 + \beta_1 \times \text{YEAR}_i + \beta_2 \times \text{UNITS}_i + \gamma_{\text{WARD}_i} \\
\gamma_{\text{WARD}_i} &\sim \mbox{Normal}(0, \tau^2) \\
\beta_0 &\sim \mbox{Normal}(0, 5) \\
\beta_1 &\sim \mbox{Normal}(0, 5) \\
\beta_2 &\sim \mbox{Normal}(0, 5) \\
\tau &\sim \mbox{Half-Cauchy}(0, 2) \\
\sigma &\sim \mbox{Half-Cauchy}(0, 2)
\end{align}
```

In this model, ($\text{SCORE}_i$) represents the evaluation score of building $i$. The mean evaluation score, ($\mu_i$), is modeled as a linear combination of the intercept ($\beta_0$), the effect of the year the building was built ($\beta_1 \times \text{YEAR}_i$), the effect of the number of units in the building ($\beta_2\times\text{UNITS}_i$), and a random effect for the location of the building ($\gamma_{\text{WARD}_i}$). Here, ($\text{YEAR}_i$) and ($\text{UNITS}_i$) are continuous predictors, while ($\gamma_{\text{WARD}_i}$) accounts for ward-specific variability in evaluation scores. The random effect ($\gamma_{\text{WARD}_i}$) is modeled as normally distributed with a mean of 0 and variance ($\tau^2$), reflecting the assumption that wards have no overall bias, but scores may vary across them.

The intercept ($\beta_0$) and coefficients ($\beta_1, \beta_2$) are assigned weakly informative normal priors with a mean of 0 and a standard deviation of 5, reflecting a prior belief that most plausible values are moderate but allowing flexibility for larger effects. The variability of the ward-level random effects ($\tau$) and the residual variability ($\sigma$) are modeled using half-Cauchy priors with a scale parameter of 2, favoring smaller values while accommodating the possibility of substantial variability. This formulation ensures that the model captures both the fixed effects of the predictors and the random effects of location, providing a robust framework for analyzing the relationship between building characteristics and evaluation scores.

This Bayesian model was chosen because it effectively accounts for the hierarchical structure of the data by including a random effect for `WARD`, capturing ward-level variability in building evaluation scores due to unobserved factors like local policies or management practices. Additionally, the Bayesian framework allows for robust inference by incorporating weakly informative priors, which regularize parameter estimates and provide a full representation of uncertainty, ensuring more reliable and interpretable results compared to traditional frequentest approaches.

We run the model in R [@citeR] using the `rstanarm` [@citeRstanarm] package of . We use the default priors from `rstanarm`.

## Model justification

Regarding the relationship between building characteristics and evaluation scores, we anticipate that newer buildings are more likely to receive higher evaluation scores. This expectation stems from the assumption that newer buildings are constructed with modern materials and methods, adhere to updated standards, and are less prone to deterioration. Conversely, older buildings may face challenges related to aging infrastructure, which could negatively impact their evaluation scores if maintenance is not consistently performed.

Similarly, we expect a positive relationship between the number of units in a building and evaluation scores. Larger buildings often have more structured management systems and resources allocated for maintenance, which can lead to better compliance with property standards. In contrast, smaller buildings may lack the economies of scale and organizational support that larger properties benefit from, potentially resulting in lower scores.

Finally, the inclusion of a random effect for 'WARD' accounts for geographic variability in evaluation scores. Factors such as neighborhood characteristics, local regulations, and differences in inspection practices across wards may influence building evaluations. This hierarchical structure ensures that the model appropriately captures these unobserved factors, making the predictions more robust and reflective of real-world dynamics.

## Model limitation

While the model provides valuable insights, it has several limitations and relies on assumptions that may not hold in all contexts. First, the assumption of a linear relationship between predictors (e.g., 'YEAR.BUILT' and 'CONFIRMED.UNITS') and the evaluation score may oversimplify complex interactions. For instance, the effect of a building's age on evaluation scores may vary depending on factors like renovation history or maintenance practices, which are not captured in the dataset. Second, the model assumes that the random effect for 'WARD' fully accounts for geographic variability, but unobserved confounders such as neighborhood socioeconomic status or differences in enforcement rigor may influence evaluation outcomes. Additionally, the use of standardized continuous predictors assumes that their scales are comparable, which may obscure meaningful differences in their effects. Finally, the model is context-specific and tailored to Toronto's RentSafeTO program, limiting its generalizability to other regions with different building standards, inspection processes, or housing markets. These limitations suggest the need for cautious interpretation and highlight areas for potential refinement in future analyses.

## Model Validation

For posterior predictive checks, in @fig-post_dist, the alignment of the posterior predictive distribution with the observed building evaluation scores suggests that the model accurately captures the patterns in the data. This indicates the reliability of the model in representing the relationship between building characteristics and evaluation scores. Additionally, @fig-post_prior compares the posterior to the prior distributions, revealing significant parameter shifts, particularly for 'YEAR.BUILT' and 'CONFIRMED.UNITS'. These shifts demonstrate that the data provided strong evidence to update the priors, though minimal changes for some parameters may reflect either limited data informativeness or initial alignment with the priors.

For Markov chain Monte Carlo (MCMC) convergence checks, @fig-trace shows trace plots for the intercept and key predictors. These plots indicate good mixing and consistent exploration of the posterior distribution, suggesting reliable convergence for these parameters. Furthermore, @fig-rhat provides the Gelman-Rubin diagnostic ($\hat{R}$), confirming that all $\hat{R}$ values are below 1.05. This ensures that the chains have converged and the samples are representative of the posterior distribution.

Additional details about the model and validation processes are provided in Appendix @sec-model-details.


# Results {#sec-result}

Our results are summarized in @tbl-modelresults. The findings align well with our expectations regarding the relationships between building characteristics and evaluation scores. To avoid multicollinearity, the model excludes one variable from each category as the reference group: 'WARD' for the ward variable and buildings with fewer than 10 units for the 'CONFIRMED.UNITS' variable. The intercept represents the estimated evaluation score for buildings in 'WARD' with fewer than 10 units, and this baseline score is [Insert Intercept Value Here].

As anticipated, 'YEAR.BUILT' has a positive coefficient, indicating that newer buildings tend to have higher evaluation scores. This suggests that newer buildings, benefiting from modern construction techniques and materials, are generally better maintained or designed to meet higher standards. The positive relationship implies that each additional year of construction is associated with a slight increase in evaluation scores, reflecting the advantages of newer infrastructure.

The number of units in a building ('CONFIRMED.UNITS') also plays a significant role. Buildings with 50–100 units or more than 100 units have higher evaluation scores compared to smaller buildings. The coefficients suggest that larger buildings, likely equipped with better management systems and resources, are better positioned to meet property standards. For instance, buildings with more than 100 units show a larger increase in evaluation scores than those with 50–100 units, further reinforcing the importance of scale in property maintenance.

Geographic location, as captured by 'WARD', exhibits notable variability. Some wards have positive coefficients, indicating higher average evaluation scores compared to the reference ward, while others show negative coefficients, suggesting lower scores. This variability underscores the influence of local factors such as neighborhood policies, inspection practices, or socioeconomic conditions on building evaluations.

@fig-modelresults1 visualizes the 90% credibility intervals for model predictors, offering insights into the certainty of each estimate. Key predictors such as 'YEAR.BUILT', 'CONFIRMED.UNITS', and certain ward effects have credibility intervals that do not overlap zero, indicating strong evidence of their influence on evaluation scores. However, some wards have wider intervals overlapping zero, highlighting uncertainty in their effects.

Together, the results underscore the significant roles of building age, size, and geographic location in determining evaluation scores. These findings suggest that newer, larger buildings in certain wards are better positioned to meet the standards set by the RentSafeTO program. The credibility intervals provide a nuanced understanding of the statistical significance and uncertainty of these predictors.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false
#| label: tbl-modelresults
#| tbl-cap: "Explanatory model Building Evaluation (n = [Insert Sample Size])"

# Load the saved Bayesian model
bayesian_model <- readRDS(file = "/cloud/project/model/bayesian_model.rds")

# Generate the model summary table
modelsummary(
  list(
    "Building Evaluation" = bayesian_model
  ),
  statistic = "mad",
)

```




# Discussion {#sec-discussion}

##  Achievements {#sec-first-point}

In this paper, we analyzed building evaluation scores from the RentSafeTO dataset to investigate the factors influencing compliance with property standards in Toronto. Using a Bayesian linear regression model, we examined how building characteristics—such as year built, number of units, and geographic location (ward)—affect evaluation scores. Our analysis incorporated random effects to account for ward-level variability and used weakly informative priors to ensure robust inference.

We selected a subset of the dataset focused on downtown Toronto and identified key predictors of building evaluation scores. The results highlight the positive relationships between newer construction, larger building size, and higher evaluation scores, while also revealing variability in scores across different wards. We visualized these relationships using regression plots, boxplots, and credibility interval plots, providing a clear picture of the factors contributing to evaluation outcomes.

Our findings emphasize the importance of building characteristics and local context in determining compliance with property standards. This paper also demonstrates the utility of Bayesian modeling for analyzing hierarchical datasets like RentSafeTO, offering flexibility in capturing both fixed effects and random variability. By focusing on actionable insights, we aim to inform property management practices and policy decisions aimed at improving housing standards in Toronto.

## About Building Maintenance and Standards {#sec-second-point}

From this analysis, we gain valuable insights into the factors that influence building maintenance and compliance with property standards. Specifically, we learn that newer buildings tend to perform better in evaluations, suggesting that modern construction practices and adherence to updated regulations play a significant role in meeting maintenance standards. This highlights the importance of updating building codes and encouraging renovations to improve compliance in older properties.

Additionally, the positive relationship between the number of units in a building and its evaluation score underscores the role of scale in property management. Larger buildings, often equipped with more structured management systems and dedicated resources, appear better positioned to meet maintenance standards compared to smaller properties. This finding suggests that economies of scale in management can lead to better overall building conditions.

Lastly, the variability in scores across different wards reveals how geographic and neighborhood factors influence compliance. Local policies, enforcement rigor, and neighborhood socioeconomic conditions likely contribute to these differences, underscoring the need for context-sensitive strategies to improve housing standards across the city.

## Limitation {#sec-third-point}

There are several limitations to consider. First, the RentSafeTO dataset only includes buildings registered under the program, excluding smaller buildings and those not covered by the bylaw. As a result, the findings may not generalize to all rental housing in Toronto, particularly for properties outside the program’s scope.

Additionally, the dataset primarily focuses on physical and structural conditions, leaving out important contextual factors such as tenant satisfaction, management practices, or broader social influences on building quality. These unobserved variables may introduce bias or limit the comprehensiveness of the analysis. Moreover, evaluation scores rely on inspections conducted by different bylaw enforcement officers, which may lead to variability due to subjective judgment or inconsistencies in enforcement practices.

The Bayesian model employed in this analysis assumes linear relationships between predictors, such as YEAR.BUILT and CONFIRMED.UNITS, and evaluation scores. While this simplification is useful for interpretability, it may overlook more complex interactions, such as the combined effects of building age and maintenance practices. Furthermore, missing values in key variables, such as evaluation scores and building characteristics, pose challenges for analysis. For instance, blank or zero scores may reflect non-evaluated cases rather than true building conditions, potentially skewing the results.

Finally, the results are context-specific and tailored to the RentSafeTO program in Toronto. They may not apply to other regions with different building standards, enforcement practices, or housing markets. These limitations emphasize the need for cautious interpretation of the findings and highlight areas for future research, such as integrating qualitative data on tenant experiences or examining the impact of inspector variability on evaluation outcomes.

## Next steps {#sec-fourth-point}

First, expanding the dataset to include a broader range of buildings, including smaller rental properties and those not registered with RentSafeTO, would provide a more comprehensive understanding of building conditions across Toronto. Incorporating data from other regions could also help generalize the findings and explore how different enforcement practices or housing markets affect evaluation outcomes.

Second, integrating additional variables, such as tenant satisfaction surveys, management practices, and neighborhood socioeconomic factors, would provide a more holistic view of what influences building quality. This could help identify factors beyond physical structure that contribute to compliance with property standards. Collecting longitudinal data on building conditions over time could also help explore the effects of renovations, policy changes, or aging on evaluation scores.

Third, standardizing inspection practices and incorporating measures to assess variability between inspectors could improve the reliability of evaluation scores. Training programs and inter-rater reliability checks could be implemented to ensure consistency in scoring across different inspectors and regions.

Finally, applying more flexible modeling approaches could capture complex interactions between predictors. For example, hierarchical or non-linear models might better reflect how building age, size, and geographic factors interact to influence evaluation scores. Exploring machine learning approaches could also help uncover hidden patterns in the data while providing robust predictions.




\newpage

\appendix

# Appendix {-}

# Data details {#sec-data-details}

## Raw data

@tbl-raw shows the preview of the raw RentSafeTo dataset.

```{r}
#| label: tbl-raw
#| tbl-cap: "Preview of the raw RentSafeTo dataset"
#| message: false
#| echo: false
#| warning: false

raw_data <- read.csv("/cloud/project/data/01-raw_data/raw_apt_data.csv")

raw_data[1:5, 1:5] |>
  kable()

raw_data[1:5, 6:10] |>
  kable()
```

## Selected variables

@tbl-feature shows the explanation of the chosen variables.

```{r}
#| label: tbl-feature
#| tbl-cap: "Selected feature of the RentSafeTo dataset"
#| message: false
#| echo: false
#| warning: false

# Define the data features
feature <- tibble(
  Data_feature = c(
    "RSN", 
    "WARD", 
    "WARDNAME", 
    "YEAR.BUILT", 
    "PROPERTY.TYPE", 
    "CONFIRMED.STOREYS", 
    "CONFIRMED.UNITS", 
    "CURRENT.BUILDING.EVAL.SCORE", 
    "PROACTIVE.BUILDING.SCORE", 
    "CURRENT.REACTIVE.SCORE"
  ),
  Data_definition = c(
    "ID number for a building", 
    "Ward where the building is located", 
    "Name of the ward", 
    "Year the building was built", 
    "Type of property owner", 
    "Number of storeys in the building", 
    "Number of units in the building", 
    "An assessment of the building's compliance with property standards, calculated by combining the reactive and proactive scores", 
    "The score of the latest proactive evaluation of a building's common areas, reflecting weighted scores of applicable categories", 
    "Total score for all applicable Orders and Notices of Violation issued for a building"
  )
)

# Create the table and apply formatting
kable(feature, format = "latex", booktabs = TRUE) %>%
  kable_styling() %>%
  column_spec(1, bold = TRUE, width = "20em") %>%  
  column_spec(2, width = "25em")  
```

Particularly, 'WARD' is the number from the 25-ward system (@ward) which is used to mark different regions in Toronto. In this research, we choose 11 and 13 which are 'University-Rosedale' and 'Toronto Centre'.

# Model details {#sec-model-details}

## Posterior predictive check

```{r}
#| label: fig-post_dist
#| fig-cap: Posterior predictive distribution for Bayesian linear model
#| echo: false
#| warning: false
#| message: false

# Load the saved model
bayesian_model <- readRDS("/cloud/project/model/bayesian_model.rds")

# Perform posterior predictive checks
pp_check(bayesian_model, type = "dens_overlay") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  ggtitle("Posterior Predictive Check: Density Overlay")
```

```{r}
#| label: fig-post_prior
#| fig-cap: Comparison of posterior and prior distributions for Bayesian linear model
#| echo: false
#| warning: false
#| message: false

prior_draws <- tibble(
  `(Intercept)` = rnorm(4000, 0, 5),
  `YEAR.BUILT` = rnorm(4000, 0, 2.5),
  `CONFIRMED.UNITS` = rnorm(4000, 0, 2.5)
) %>%
  pivot_longer(cols = everything(), names_to = "Parameter", values_to = "Value") %>%
  mutate(Type = "Prior")

# Extract posterior samples
posterior_draws <- as_draws_df(as.matrix(bayesian_model)) %>%
  select(`(Intercept)`, `YEAR.BUILT`, `CONFIRMED.UNITS`) %>%
  pivot_longer(cols = everything(), names_to = "Parameter", values_to = "Value") %>%
  mutate(Type = "Posterior")

# Combine prior and posterior samples
prior_posterior <- bind_rows(prior_draws, posterior_draws)

# Plot comparison of posterior and prior
ggplot(prior_posterior, aes(x = Parameter, y = Value, color = Type, fill = Type)) +
  stat_halfeye(alpha = 0.6, adjust = 0.5, width = 0.7, justification = -0.3) +
  theme_minimal() +
  coord_flip() +
  labs(
    title = "Comparison of Posterior and Prior Distributions",
    x = "Parameter",
    y = "Value",
    color = "Distribution Type",
    fill = "Distribution Type"
  ) +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(hjust = 1, vjust = 0.5),
    text = element_text(size = 8)
  ) +
  guides(color = guide_legend(ncol = 4))
```

In @fig-post_dist, we compare the model's predicted values to the observed data. This involves overlaying the posterior predictive distribution on the observed data distribution. The purpose of this check is to evaluate whether the model can reproduce patterns and variability seen in the actual evaluation scores. The results indicate a good fit, as the predicted and observed distributions align closely, suggesting the model captures the underlying structure of the data effectively. This validation supports the model's reliability in representing the relationship between building characteristics and evaluation scores.

@fig-post_prior compares the posterior with the prior distributions for key parameters in the Bayesian model, including the intercept, 'YEAR.BUILT', and 'CONFIRMED.UNITS'. The plot uses overlapping density curves to illustrate how the data influenced the parameter estimates. The posterior distributions shift away from the priors, indicating that the data provided substantial evidence to update the parameter values. For instance, the posterior for 'YEAR.BUILT' narrows and centers around values informed by the observed data, reflecting its significant role in predicting evaluation scores.

## Markov chain Monte Carlo Convergence Check

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-trace
#| fig-cap: "Trace plot of intercept and predictors"
#| fig-subcap: ["Trace plot of Intercept", "Trace plot of YEAR.BUILT", "Trace plot of CONFIRMED.UNITS"]
#| layout-ncol: 2

# Load the saved model
bayesian_model <- readRDS("/cloud/project/model/bayesian_model.rds")

# Trace plots for key fixed effects
plot(bayesian_model, "trace", "(Intercept)")
plot(bayesian_model, "trace", "YEAR.BUILT")
plot(bayesian_model, "trace", "CONFIRMED.UNITS")
```

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-rhat
#| fig-cap: "Rhat plot"

plot(bayesian_model, "rhat")
```


# 90% Credibility Interval {#sec-credibility-interval}

@fig-modelresults is a 90% credibility interval plot.

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false
#| label: fig-modelresults
#| fig-cap: "90% Credibility Intervals for Building Evaluation Model Predictors"

# Load the saved model
bayesian_model <- readRDS("/cloud/project/model/bayesian_model.rds")

# Generate 90% credibility interval plot
modelplot(bayesian_model, conf_level = 0.90, size = 0.2) +
  labs(
    x = "90% Credibility Interval",
    title = "Credible Intervals for Building Evaluation Model Predictors"
  ) +
  theme(
    axis.text.y = element_text(size = 6),
    axis.title.y = element_blank() # Remove y-axis title for a cleaner look
  )

```


@fig-trace is a trace plot. It shows the sampling paths for key fixed effects, including the intercept, 'YEAR.BUILT', and 'CONFIRMED.UNITS', across all Markov chains. The plots demonstrate good mixing, with chains overlapping and exploring the posterior distribution consistently without any visible drifts or irregular patterns. This suggests that the sampling process converged for these parameters, providing reliable estimates for the posterior distributions.

@fig-rhat is a Rhat plot. It shows the Gelman-Rubin diagnostic $\hat{R}$ for all model parameters, which assesses convergence across Markov chains. All $\hat{R}$ values are close to 1 and below the commonly accepted threshold of 1.05, indicating that the chains have converged, and the samples are representative of the posterior distribution. This suggests that the model has achieved satisfactory convergence, and the parameter estimates can be trusted for inference.


# Survey



# References
